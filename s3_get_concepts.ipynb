{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17001bd9-da10-4fe7-a3d4-9c5174944296",
   "metadata": {},
   "source": [
    "### load all the processed preprint papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18ebad34-ea44-4336-8665-ceabec4c5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('all_paper_info_lists.pkl'):\n",
    "    # open the existing pickle file for reading\n",
    "    with open('all_paper_info_lists.pkl', 'rb') as f:\n",
    "        all_paper_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4475b-b22e-417d-8f74-64045ea27f90",
   "metadata": {},
   "source": [
    "### put title and abstract together, store in to string list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4ce1c",
   "metadata": {},
   "source": [
    "文本合并：标题 + 空格 + 摘要\n",
    "\n",
    "大小写统一：全部转为小写字母\n",
    "\n",
    "字符清洗：\n",
    "\n",
    "去除换行符和连字符\n",
    "\n",
    "处理德语变音符号（ä→ae, ö→oe, ü→ue）\n",
    "\n",
    "移除所有引号和撇号\n",
    "\n",
    "规范化空格（多个空格合并为单个空格）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd905ebe-207c-4f63-b869-5e4434343ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_article_string(article):\n",
    "    \n",
    "    curr_title=article[1] #'title'\n",
    "    curr_abstract=article[2] #'abstract'\n",
    "    \n",
    "    replace_pairs=[['\\n',' '],['-',' '],[' \\\" a','oa'],['\\\" a','ae'],['\\\"a','ae'],[' \\\" o','oe'],['\\\" o','oe'],['\\\"o','oe'],[' \\\" u','ue'],\n",
    "                   ['\\\" u','ue'],['\\\"u','ue'],[' \\' a','a'],[' \\' e','e'],[' \\' o','o'],[\"\\' \", \"\"],[\"\\'\", \"\"],['  ',' '],['  ',' ']]\n",
    "    \n",
    "    article_string=(curr_title +' '+ curr_abstract).lower()\n",
    "    \n",
    "    for rep_pair in replace_pairs:\n",
    "        #print(rep_pair)\n",
    "        \n",
    "        article_string=article_string.replace(rep_pair[0],rep_pair[1])\n",
    "        #print(article_string)\n",
    "        #print('\\n')\n",
    "    \n",
    "    return article_string\n",
    "\n",
    "def get_all_paper_strings(article_lists):\n",
    "\n",
    "    if os.path.exists('all_paper_string_lists.pkl'):\n",
    "        with open(\"all_paper_string_lists.pkl\", \"rb\") as f:\n",
    "            all_paper_strings = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        all_paper_strings=[]\n",
    "        cc=0\n",
    "        for id_of_paper in range(len(article_lists)):\n",
    "            cc+=1\n",
    "            if (cc%300000)==0:\n",
    "                print(str(cc)+'/'+str(len(article_lists)))\n",
    "\n",
    "            all_paper_strings.append(get_single_article_string(article_lists[id_of_paper]))\n",
    "\n",
    "        with open(\"all_paper_string_lists.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_paper_strings, f)\n",
    "    \n",
    "    return all_paper_strings\n",
    "\n",
    "\n",
    "\n",
    "all_article_strings=get_all_paper_strings(all_paper_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3b05e-a8eb-4e1a-a4a6-273f702eac16",
   "metadata": {},
   "source": [
    "### Get Concepts from RAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec52d8f",
   "metadata": {},
   "source": [
    "nltk.corpus.stopwords.words('english') 需要下载停用词数据，但系统中没有这个资源。\n",
    "\n",
    "在运行代码之前，先执行以下命令下载必要的NLTK数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845be865-8db6-4956-ade5-918b191954dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     UNEXPECTED_EOF_WHILE_READING] EOF occurred in\n",
      "[nltk_data]     violation of protocol (_ssl.c:1000)>\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'wordnet' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/corpora/wordnet.zip>:   <urlopen error\n",
      "[nltk_data]     [WinError 10054] 远程主机强迫关闭了一个现有的连接。>\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK数据下载完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 下载停用词数据\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 下载词形还原器需要的数据\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 下载分词需要的数据\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(\"NLTK数据下载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56391e",
   "metadata": {},
   "source": [
    "RAKE算法需要NLTK的punkt分词器来分割句子，但缺少punkt_tab语言包。\n",
    "下载所有必要的NLTK数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984039be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始下载NLTK必要数据包...\n",
      "已下载: stopwords\n",
      "已下载: wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\张士涵\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m required_packages:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m         nltk\u001b[38;5;241m.\u001b[39mdownload(package)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m已下载: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:774\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    766\u001b[0m     print_to(\n\u001b[0;32m    767\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[0;32m    768\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;66;03m# Error messages\u001b[39;00m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[0;32m    777\u001b[0m         show(msg\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_package(info, download_dir, force)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:710\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    708\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, info\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[1;32m--> 710\u001b[0m     s \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m)  \u001b[38;5;66;03m# 16k blocks.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(s)\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# 下载所有必要的NLTK数据包\n",
    "required_packages = [\n",
    "    'stopwords',      # 停用词\n",
    "    'wordnet',        # 词形还原\n",
    "    'omw-1.4',        # 多语言词网\n",
    "    'punkt',          # 基础分词器\n",
    "    'punkt_tab'       # 分词器语言包（解决当前错误）\n",
    "]\n",
    "\n",
    "print(\"开始下载NLTK必要数据包...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        nltk.download(package)\n",
    "        print(f\"已下载: {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"下载 {package} 失败: {e}\")\n",
    "\n",
    "print(\"NLTK数据下载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b36783",
   "metadata": {},
   "source": [
    "使用RAKE算法从所有论文文本中批量提取论文标题和摘要中的关键概念短语，使用停用词作为分隔符，将文本分割成候选短语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53875ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/2848279\n",
      "200000/2848279\n",
      "300000/2848279\n",
      "400000/2848279\n",
      "500000/2848279\n",
      "600000/2848279\n",
      "700000/2848279\n",
      "800000/2848279\n",
      "900000/2848279\n",
      "1000000/2848279\n",
      "1100000/2848279\n",
      "1200000/2848279\n",
      "1300000/2848279\n",
      "1400000/2848279\n",
      "1500000/2848279\n",
      "1600000/2848279\n",
      "1700000/2848279\n",
      "1800000/2848279\n",
      "1900000/2848279\n",
      "2000000/2848279\n",
      "2100000/2848279\n",
      "2200000/2848279\n",
      "2300000/2848279\n",
      "2400000/2848279\n",
      "2500000/2848279\n",
      "2600000/2848279\n",
      "2700000/2848279\n",
      "2800000/2848279\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Metric, Rake\n",
    "from collections import Counter\n",
    "\n",
    "starting_time = time.time()\n",
    " \n",
    "wnl=WordNetLemmatizer()\n",
    "\n",
    "num_of_abstracts=len(all_paper_lists)\n",
    "\n",
    "personal_stop_list=['presents','us','show','one','two','three','describes','new','approach','many','introduces','http','also','whose', 'prove','select ','take']\n",
    "\n",
    "nltk_stop_list=nltk.corpus.stopwords.words('english')\n",
    "full_stop_list=nltk_stop_list + personal_stop_list\n",
    "\n",
    "\n",
    "all_concepts_from_rake=[]\n",
    "cc=0\n",
    "for id_of_abstract in range(num_of_abstracts):\n",
    "    cc+=1\n",
    "    if (cc%100000)==0:\n",
    "        print(str(cc)+'/'+str(num_of_abstracts))\n",
    "    \n",
    "            \n",
    "    single_string = get_single_article_string(all_paper_lists[id_of_abstract])\n",
    "    \n",
    "    r = Rake(stopwords=full_stop_list, ranking_metric=Metric.WORD_DEGREE, min_length=2, include_repeated_phrases=False)\n",
    "\n",
    "    r.extract_keywords_from_text(single_string)\n",
    "    ll=r.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    all_concepts_from_rake.extend(ll)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
