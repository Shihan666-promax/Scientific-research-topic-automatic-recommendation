{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cb583d-abc8-4a11-8334-448a6d0afcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "if os.path.exists('..\\\\data\\\\all_concepts_from_rake.pkl'):\n",
    "# open the existing pickle file for reading\n",
    "    with open('..\\\\data\\\\all_concepts_from_rake.pkl', 'rb') as f:\n",
    "        all_concepts_from_rake = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa89bacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33.0, 'reduced estimated total intracranial volume'), (29.0, 'dementia related socioeconomic'), (27.0, 'uncovering sex related pathways'), (27.0, 'normalized whole brain volume'), (26.0, 'resultslower education affected dementia')]\n"
     ]
    }
   ],
   "source": [
    "######我加的\n",
    "###############打印出来的格式是\n",
    "# 得分：关键词\n",
    "print(all_concepts_from_rake[:5]) # 如果是列表，打印前5个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f46a27",
   "metadata": {},
   "source": [
    "对RAKE算法提取的关键词进行后处理和频率统计，采用分批处理来解决内存溢出问题，最终结果：final_all_key_concepts_counter.pkl（包含所有关键词及其出现频率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082df8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存溢出，拆开处理\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 初始化词形还原器\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# 禁用字符\n",
    "forbidden_chars = ['.', ',', '~', 'http', '&', '(', ')', '$', ' et al', '^', '{', '}', '/']\n",
    "\n",
    "\n",
    "# 分批次处理函数\n",
    "def process_batch(batch_start, batch_end, all_concepts_from_rake, all_key_concepts):\n",
    "    # 处理一批数据\n",
    "    for id_concept, curr_concept_with_score in enumerate(all_concepts_from_rake[batch_start:batch_end]):\n",
    "        if curr_concept_with_score[0] > 1:  # 过滤掉得分小于1的概念\n",
    "            curr_concept = curr_concept_with_score[1]\n",
    "\n",
    "            if not any(char in curr_concept for char in forbidden_chars):\n",
    "                curr_concept_split = curr_concept.split(' ')\n",
    "                curr_concept_split[-1] = wnl.lemmatize(curr_concept_split[-1])\n",
    "                curr_concept_split[-1] = curr_concept_split[-1].replace('matroids', 'matroid')\n",
    "                curr_concept_singularized = ' '.join(curr_concept_split)\n",
    "                all_key_concepts.append(curr_concept_singularized)\n",
    "    return all_key_concepts\n",
    "\n",
    "\n",
    "# 主要流程\n",
    "def process_concepts_in_batches(all_concepts_from_rake, batch_size=100000):\n",
    "    total_concepts = len(all_concepts_from_rake)\n",
    "    all_key_concepts = []\n",
    "\n",
    "    # 分批次处理\n",
    "    for batch_start in range(0, total_concepts, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_concepts)\n",
    "        print(f\"Processing batch {batch_start} to {batch_end}...\")\n",
    "\n",
    "        # 处理当前批次\n",
    "        all_key_concepts = process_batch(batch_start, batch_end, all_concepts_from_rake, all_key_concepts)\n",
    "\n",
    "        # 每批次处理完后，保存当前结果\n",
    "        partial_count = Counter(all_key_concepts)\n",
    "        with open(f\"partial_all_key_concepts_counter_{batch_start}.pkl\", \"wb\") as output_file:\n",
    "            pickle.dump(partial_count, output_file)\n",
    "\n",
    "    # 最后合并所有批次结果\n",
    "    final_count = Counter(all_key_concepts)\n",
    "    with open(\"final_all_key_concepts_counter.pkl\", \"wb\") as output_file:\n",
    "        pickle.dump(final_count, output_file)\n",
    "\n",
    "\n",
    "# 假设 all_concepts_from_rake 已经加载好\n",
    "# 这里使用示例数据代替\n",
    "#all_concepts_from_rake = [(2, \"machine learning\")]\n",
    "process_concepts_in_batches(all_concepts_from_rake, batch_size=40000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ca2a32-b133-4c66-a403-3f523653d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/79883107\n",
      "2000000/79883107\n",
      "4000000/79883107\n",
      "6000000/79883107\n",
      "8000000/79883107\n",
      "10000000/79883107\n",
      "12000000/79883107\n",
      "14000000/79883107\n",
      "16000000/79883107\n",
      "18000000/79883107\n",
      "20000000/79883107\n",
      "22000000/79883107\n",
      "24000000/79883107\n",
      "26000000/79883107\n",
      "28000000/79883107\n",
      "30000000/79883107\n",
      "32000000/79883107\n",
      "34000000/79883107\n",
      "36000000/79883107\n",
      "38000000/79883107\n",
      "40000000/79883107\n",
      "42000000/79883107\n",
      "44000000/79883107\n",
      "46000000/79883107\n",
      "48000000/79883107\n",
      "50000000/79883107\n",
      "52000000/79883107\n",
      "54000000/79883107\n",
      "56000000/79883107\n",
      "58000000/79883107\n",
      "60000000/79883107\n",
      "62000000/79883107\n",
      "64000000/79883107\n",
      "66000000/79883107\n",
      "68000000/79883107\n",
      "70000000/79883107\n",
      "72000000/79883107\n",
      "74000000/79883107\n",
      "76000000/79883107\n",
      "78000000/79883107\n"
     ]
    }
   ],
   "source": [
    "## 原版\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Metric, Rake\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('wordnet') Download necessary NLTK datasets\n",
    "wnl=WordNetLemmatizer()\n",
    "all_key_concepts=[]\n",
    "forbidden_chars=['.',',','~','http','&','(',')','$', ' et al','^','{','}','/']\n",
    "\n",
    "for id_concept, curr_concept_with_score in enumerate(all_concepts_from_rake):\n",
    "    \n",
    "    if curr_concept_with_score[0]>1:\n",
    "        curr_concept=curr_concept_with_score[1]\n",
    "\n",
    "        if (id_concept%2000000)==0:\n",
    "            print(str(id_concept)+'/'+str(len(all_concepts_from_rake)))\n",
    "\n",
    "        if not any(char in curr_concept for char in forbidden_chars):\n",
    "            curr_concept_split=curr_concept.split(' ') \n",
    "            curr_concept_split[-1]=wnl.lemmatize(curr_concept_split[-1])\n",
    "            curr_concept_split[-1]=curr_concept_split[-1].replace('matroids','matroid')\n",
    "            curr_concept_singularized=' '.join(curr_concept_split)\n",
    "            all_key_concepts.append(curr_concept_singularized)\n",
    "        \n",
    "all_concepts_count=Counter(all_key_concepts).most_common()\n",
    "with open(\"all_key_concepts_counter.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(all_concepts_count, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69556764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_concepts_count:  29338139\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# 从文件中加载 all_concepts_count\n",
    "def load_concepts_count(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        all_concepts_count = pickle.load(file)\n",
    "    return all_concepts_count\n",
    "\n",
    "# 读取 final_all_key_concepts_counter.pkl 中的数据\n",
    "all_concepts_count = load_concepts_count(\"final_all_key_concepts_counter.pkl\")\n",
    "\n",
    "# 获取当前日期\n",
    "curr_date = datetime.date.today()\n",
    "\n",
    "# 打印概念计数的长度\n",
    "print(\"all_concepts_count: \", len(all_concepts_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36701182",
   "metadata": {},
   "source": [
    "筛选出高频3词+短语（≥6次），筛选出高频2词短语（≥9次）  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5179b1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_concepts_count:  29338139\n",
      "3:  405954\n",
      "total:  875840\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "\n",
    "curr_date = datetime.date.today()\n",
    "print(\"all_concepts_count: \", len(all_concepts_count))\n",
    "\n",
    "full_concept_list = []\n",
    "\n",
    "num3 = 6\n",
    "num2 = 9\n",
    "\n",
    "# 确保 all_concepts_count 是字典或者Counter，且使用 .items() 解包\n",
    "for letter, count in all_concepts_count.items():  # 修改为 items() 来解包\n",
    "    if len(letter.split(' ')) >= 3 and count >= num3:\n",
    "        full_concept_list.append(letter)\n",
    "print(\"3: \", len(full_concept_list))\n",
    "\n",
    "# 第二轮筛选：长度为2，计数>= num2\n",
    "for letter, count in all_concepts_count.items():  # 使用 items() 来解包\n",
    "    if len(letter.split(' ')) == 2 and count >= num2:\n",
    "        full_concept_list.append(letter)\n",
    "\n",
    "print(\"total: \", len(full_concept_list))\n",
    "\n",
    "# 保存筛选后的结果\n",
    "with open(f\"full_concepts_{num3}_{num2}.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(full_concept_list, output_file)\n",
    "\n",
    "# 也将结果保存为文本文件\n",
    "with open(f\"full_concepts_{num3}_{num2}.txt\", \"a\") as f:\n",
    "    for ii in range(len(full_concept_list)):\n",
    "        f.write(full_concept_list[ii] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037ddb2-bbf5-49af-a6af-a7a98823d992",
   "metadata": {},
   "source": [
    "### improve concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ddc536",
   "metadata": {},
   "source": [
    "对关键词概念进行过滤和清洗，包含字符、文档格式标记和奇怪缩写\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a30c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################这个函数未定义，我们自己写的###########################\n",
    "def filter_words(concept, filter_list):\n",
    "    \"\"\"\n",
    "    检查给定的概念（concept）是否包含在过滤字符列表（filter_list）中的任意字符。\n",
    "    \n",
    "    Parameters:\n",
    "        concept (str): 需要检查的概念字符串。\n",
    "        filter_list (list): 需要过滤的字符或词语列表。\n",
    "        \n",
    "    Returns:\n",
    "        bool: 如果概念包含任何过滤字符，则返回 True，否则返回 False。\n",
    "    \"\"\"\n",
    "    for filter_word in filter_list:\n",
    "        if filter_word in concept:  # 如果概念中包含任何过滤词汇\n",
    "            return False  # 返回 True 表示该概念不符合要求，应该移除\n",
    "    return True  # 如果没有任何过滤词汇，返回 False，表示保留该概念\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1591f8ba-2efe-4762-b928-25618b13b2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/875840\n",
      "40000/875840\n",
      "60000/875840\n",
      "80000/875840\n",
      "100000/875840\n",
      "120000/875840\n",
      "140000/875840\n",
      "160000/875840\n",
      "180000/875840\n",
      "200000/875840\n",
      "220000/875840\n",
      "240000/875840\n",
      "260000/875840\n",
      "280000/875840\n",
      "300000/875840\n",
      "320000/875840\n",
      "340000/875840\n",
      "360000/875840\n",
      "380000/875840\n",
      "400000/875840\n",
      "420000/875840\n",
      "440000/875840\n",
      "460000/875840\n",
      "480000/875840\n",
      "500000/875840\n",
      "520000/875840\n",
      "540000/875840\n",
      "560000/875840\n",
      "580000/875840\n",
      "600000/875840\n",
      "620000/875840\n",
      "640000/875840\n",
      "660000/875840\n",
      "680000/875840\n",
      "700000/875840\n",
      "720000/875840\n",
      "740000/875840\n",
      "760000/875840\n",
      "780000/875840\n",
      "800000/875840\n",
      "820000/875840\n",
      "840000/875840\n",
      "860000/875840\n",
      "Concepts: 875840 ; Store: 873992; Remove: 1848 \n"
     ]
    }
   ],
   "source": [
    "# starting_time = time.time()\n",
    "\n",
    "filter_chars=['\">','=\"','<\"','>','<','=',']>','``','``\\\\','\\\\[','\\\\|','><','=\\\\','[\\\\',\n",
    "              '\"\\\\','\\\\%','|\\\\','+\\\\','\\\\`','\\\\!','?!','\\\\!\\\\','%;',']:',':\\\\','++']\n",
    "\n",
    "weird_words=['o_fig','o_fig_display_l','no_fig','o_linksmallfig','o_tbl','_t','u_']\n",
    "\n",
    "\n",
    "filter_chars.extend(weird_words)\n",
    "\n",
    "new_reduced_list=[]\n",
    "removed_list=[]\n",
    "cc=0\n",
    "\n",
    "for curr_concept in full_concept_list:\n",
    "    \n",
    "    cc+=1\n",
    "\n",
    "    if (cc%20000)==0:\n",
    "        print(str(cc)+'/'+str(len(full_concept_list)))\n",
    "\n",
    "    if filter_words(curr_concept, filter_chars): #filter_words(curr_concept, filter_chars, filter_word_list)\n",
    "        \n",
    "        new_reduced_list.append(curr_concept)\n",
    "    else:\n",
    "        #print('remove : ', curr_concept)\n",
    "        removed_list.append(curr_concept)\n",
    "\n",
    "        \n",
    "print(\"Concepts: {:d} ; Store: {:d}; Remove: {:d} \".format(len(full_concept_list), len(new_reduced_list),len(removed_list)))\n",
    "# elapsed_time = time.time() - starting_time\n",
    "# print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f47ae",
   "metadata": {},
   "source": [
    "定义复杂的过滤规则体系\n",
    "按词汇位置过滤\n",
    "开头词过滤 (filter_start_words)：去除以\"cause\", \"shows\", \"study\"等开头的关键词\n",
    "\n",
    "结尾词过滤 (filter_last_words)：去除以\"show\", \"data\", \"class\"等结尾的关键词\n",
    "\n",
    "特定长度开头词 (filter_start_words_length)：对2词短语的特殊开头词过滤\n",
    "\n",
    "按词汇类型过滤\n",
    "通用停用词 (filter_lists)：描述性动词、形容词等（\"described\", \"used\", \"better\"）\n",
    "\n",
    "副词过滤 (filter_other_adverb_lists)：\"successfully\", \"directly\", \"significantly\"等\n",
    "\n",
    "通用名词过滤 (filter_general_name_lists)：\"patients\", \"country\", \"school\"等\n",
    "\n",
    "时间单位过滤：月份、年份、计量单位等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc5ac28-651f-45b3-b4fb-4a2d050b926c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts: 873992 ; Store: 613794; Remove: 260198 \n",
      "Elapsed time: 12.31 seconds\n",
      "\n",
      "\n",
      "05-12-2025 00:51:13; Concepts: 613794 \n",
      "create text and store information.\n",
      "create text and store information.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "def is_valid_year(string):\n",
    "    pattern = re.compile(r'^\\d{4}$')\n",
    "    if pattern.match(string) is not None:\n",
    "        year = int(string)\n",
    "        return year <= 2025\n",
    "    return False\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "#filter contains the word and also the length is two\n",
    "filter_start_words_length=['fully','recall','sudden','change','least','ultimately','label','essentially','almost']\n",
    "\n",
    "filter_start_words=['cause','performs','around','art','currently','current','sim','leq','carefully','simeq','giving','becomes',\n",
    "                   'geqslant','geq','input','completely','000','highly','sigma','arbitrary','hence','study','actual',\n",
    "                   'removing','ideal','fixed','theoretical','shows','estimates','correct','still','detailed','less','reduce',\n",
    "                    'reduced','ensure','effectively','detect','deforming','draw','newly','following','manner','always','table',\n",
    "                   'besides','italian','japanese','poor','although','federal','predicting','youtube','pregnant','relatively',\n",
    "                   'gain','quite','particularly','actually','thereby','thoroughly','finally','perhaps','subsequently','existing',\n",
    "                   'unlike','pushes','places','update','causes','bringing','similar','write','maximizing','generating','solve',\n",
    "                    'indeed','spread','ask','truly','surprisingly','furthermore','additionally','break','search','eventually',\n",
    "                    'leads','learning']\n",
    "\n",
    "\n",
    "\n",
    "filter_last_words=['show', 'showed', 'showing','conference','caused','within','pdfs','employed','requirement','optical',\n",
    "                   'km','data','alone','focus','sec','market','focused','collected','led','class','asked','larger','upgrade',\n",
    "                   'increase','onto','m_','reflects','outcome','output','order','performs','ness','way','kev','solves','close',\n",
    "                   'manner','according','developed','automatic','generalizes','relies','exceeds','aiming','dimensional','closely',\n",
    "                  'exceeding','project','globally','smaller','input','u','richer','via','db','mm','governing','governed','like',\n",
    "                   'analyzing','indicates','indicate','million','reveals','away','sigma','equivalent','linearly','seen','resolve',\n",
    "                  'impossible','need','together','perspective','solve','one','growth','member','involving','generated','discovered',\n",
    "                  'surely','along','ii','iii','driver','subtasks','company','province','council','woman','indicative','aim',\n",
    "                   'label','us','start','try','go','almost','update','consists','forward','ahead','similar','spread','asks',\n",
    "                   'observes','support','corroborate','insight','behind']\n",
    "\n",
    "\n",
    "string_to_remove=['log log n','ra e','b ball','2 m_q','4 sca','high j','area 1','kappa 0','sim 2 3','0 ap',\n",
    "               'least 1 1','2 coefficient','2 5 time','h2 0 0','4e 4']\n",
    "\n",
    "filter_useless_strings=['computation cost','real world', 'real time', 'standard deviation', 'technical challenge',\n",
    "                       ' times higher','high applied ','open source']\n",
    "\n",
    "\n",
    "filter_words_fixed_len2=['support','strongly','jointly','start','independently','severe','go','iteratively','move','relevant',\n",
    "                        'properly','empirically','main','dynamically','far','leading']\n",
    "\n",
    "filter_lists=['described','describe','extends', 'studied', 'paper', 'shown', 'much', 'among', 'established', 'good', 'first', 'encodes', 'certain', \n",
    "              'increasing', 'various', 'used', 'unknown', 'lead','even', 'outperforms', 'ago', 'uses', 'entire', 'selected', \n",
    "              'last', 'within', 'present', 'yet', 'towards', 'based', 'well', 'achieve','achieves', 'achieved', 'regarding',\n",
    "              'yields', 'review', 'may', 'better', 'performed', 'quot', 'learns', 'combined', 'practical','obtain', 'obtaining', \n",
    "              'patient', 'attains', 'year', 'vast', 'define', 'mev', 'provides', 'allows', 'generates', 'compared', 'reproducing',\n",
    "              'improve', 'operates', 'years', 'trained', 'learn', 'would', 'beyond', 'derived', 'available', 'otherwise', 'every', \n",
    "              'best', 'result','resulting', 'make','makes', 'made', 'becoming', 'previous', 'arises', 'applying', 'obtained', 'poses', \n",
    "              'lacks', 'called', 'associated', 'proposes', 'brief', 'another', 'scores', 'promising', 'demonstrate', 'times',\n",
    "              'challenging', 'act', 'suited', 'tremendous', 'beats', 'ral', 'work', 'multi', 'therefore', 'looking', 'important', \n",
    "              'extend', 'volunteer', 'could', 'recent', 'fit', 'produce', 'become', 'needed', 'providing', 'taken','turn',\n",
    "               'observed', 'reviewer', 'phi', 'unless', 'apply', 'using', 'given', 'significant', 'proposed', 'published', \n",
    "               'give', 'sometimes', 'train', 'use', 'debate', 'applies', 'attracted', 'case', 'adding', 'includes', 'developed',\n",
    "              'approximation','approx','approximate','whole','thus','characterize','surviving','degree','contain','relies','done',\n",
    "              'determine','even','occurs','occur','avoid','estimate','estimated','reporting','task','hour','despite',\n",
    "              'confirms','confirm','describing','release','progress','reported','several','run','running','degrees','derive',\n",
    "              'derived','day','serious','introduce','introduced','institutional','without','evolve','deg','per','remain','remains',\n",
    "              'proving','provide','office','university','subtask','appears','display','displayed','toward','takes','additional',\n",
    "              'appendix','discuss','discussed','let','cannot','yield','gives','output','allowing','allow','june','submitted',\n",
    "              'context','testing','often','requires','require','including','required','demonstrates','raised','enables',\n",
    "              'making','help','helping','helps','tested','find','finding','gev','suggests','suggesting','construct','reformulated',\n",
    "              'depends','reformulate','facilitating','future','might','ignored','ignore','typical','contributes','contribute',\n",
    "              'revealed','included','investigate','investigated','investigates','different','next','excellent','faster','affected',\n",
    "              'remarkable','expressed','found','improves','chosen','enable','cases','identify','modeled','suitable','utilizing',\n",
    "              'predicted','predict','aged','healthy','exhibit','drew','justify','suggest','depend','behave','behaves','reproduces',\n",
    "              'compute','happens','happen','calculate','perform','reduces','reduce','rise','unexpected','predicts','classify',\n",
    "             'reproduce','vary','explore','analyze','explain','evaluate','affect','instead','works','bring','lack','puts','put',\n",
    "             'remove','differs','differ','impose','imposes','develop','moves','consist','cause','brought','look','generate',\n",
    "              'hold','holds','get','imply','illustrate','verify','implement','understood','understand','generalize','generalizes',\n",
    "             'interpret','interprets','proven','follows','follow','interpret','acts','taking','becomes','account','observe','received',\n",
    "             'know','reveal','reveals','validate','validates','agree','agrees','establish','creates','create','breaks','compare',\n",
    "              'compares','consider','considers','yielding','yielded','offer','offers','provided','reaching']\n",
    "\n",
    "filter_other_adverb_lists=['weekly','likely','unlikely','visually','successfully','previously','directly','indirectly','manually','seriously',\n",
    "                   'explicitly','significantly','autonomously', 'approximately','optimally','widely','possibly','simultaneously',\n",
    "                   'extremely','nearly','closely','largely','naturally','simply','solely','early','clearly','slightly','possible',\n",
    "                  'typically','must','overall','either','unusually','usually','unusual','potential','potentially','rapidly','recently',\n",
    "                   'rather','sufficiently','since','selectively','whether','appropriately','reliably','similarly','effectively','drastically',\n",
    "                   'accurately','correctly','adequately','formally','systematically','efficiently','intuitively','exactly','together',\n",
    "                    'due','everywhere','whereas','considerably','include','immediately','automatically','numerically','however',\n",
    "                    'especially','poorly','particularly','easily','quickly','experimentally','briefly']\n",
    "\n",
    "filter_general_name_lists=['german','russian','college','patients','country','month','workshop','participant','preprints',\n",
    "                           'sections','percentage','section','table','version','institution','week','percent','minute','user',\n",
    "                          'worker','students','investor','property','process','example','examples','million','bug','chapter',\n",
    "                          'proposal','school','indian','european','american','government','chinese','city','uk','hospital',\n",
    "                          'department','people','physician','github','presentation','english','license','python','summer',\n",
    "                           'researchers','plan','researcher','months','days','players','results']\n",
    "\n",
    "filter_months_lists = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "filter_units_lists=['myr','kv','kb','nm','wt','kg','au','kw','gk','mw','mv','ev','mhz','hz','khz','micron','cm2','fb','ghz','cm',\n",
    "             'nu','gyr','pc','km','km2','kpc']\n",
    "\n",
    "filter_symbol_lists=['%:','%]',\"\\\\\",'**','\\\\;\\\\',']\\\\',':[',']]',']+','\\\\\\\\','\\\\]']\n",
    "\n",
    "filter_lists.extend(filter_other_adverb_lists)\n",
    "filter_lists.extend(filter_general_name_lists)\n",
    "filter_lists.extend(filter_months_lists)\n",
    "filter_lists.extend(filter_units_lists)\n",
    "filter_lists.extend(filter_symbol_lists)\n",
    "\n",
    "\n",
    "improved_concept_list=[]\n",
    "    \n",
    "for one_concept in new_reduced_list:\n",
    "    \n",
    "    separated_words=one_concept.split()\n",
    "    do_remove=0\n",
    "    \n",
    "    for word in separated_words:\n",
    "        if word in filter_lists:\n",
    "            do_remove=1\n",
    "            break\n",
    "        if is_valid_year(word):\n",
    "            do_remove=1\n",
    "            break            \n",
    "            \n",
    "        #for ii in range(len(filter_restricted1)):\n",
    "            #if word==filter_restricted1[ii] and (filter_restricted2[ii] in separated_words):\n",
    "                #do_remove=1\n",
    "                #break\n",
    "    \n",
    "    for ii in range(len(filter_useless_strings)):\n",
    "        if filter_useless_strings[ii] in one_concept:\n",
    "            do_remove=1\n",
    "            break\n",
    "\n",
    "    if len(separated_words)==2:\n",
    "        for word in separated_words:\n",
    "            if word in filter_words_fixed_len2:\n",
    "                do_remove=1\n",
    "                break\n",
    "\n",
    "    if separated_words[0].isnumeric() and len(separated_words)==2: # 2 books, 4 objects, etc...\n",
    "        do_remove=1\n",
    "    \n",
    "       \n",
    "    if separated_words[0] in filter_start_words:\n",
    "        do_remove=1\n",
    "            \n",
    "    if separated_words[0] in filter_start_words_length and len(separated_words)==2 :\n",
    "        do_remove=1\n",
    "        \n",
    "    if separated_words[-1] in filter_last_words:\n",
    "        do_remove=1\n",
    "\n",
    "\n",
    "    if do_remove==0:\n",
    "        if one_concept not in string_to_remove:\n",
    "            improved_concept_list.append(one_concept)\n",
    "        \n",
    "print(\"Concepts: {:d} ; Store: {:d}; Remove: {:d} \".format(len(new_reduced_list), len(improved_concept_list),len(new_reduced_list)-len(improved_concept_list)))\n",
    "elapsed_time = time.time() - starting_time\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "print('\\n')\n",
    "now_time =  datetime.datetime.now()\n",
    "formatted_time = now_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"{}; Concepts: {:d} \".format(formatted_time,len(improved_concept_list)))\n",
    "\n",
    "\n",
    "\n",
    "## here is just for a backup\n",
    "f = open(f\"full_concepts_stored_lists_{num3}_{num2}_improved.txt\", \"a\")\n",
    "for ii in range(len(improved_concept_list)):\n",
    "    f.write(improved_concept_list[ii]+'\\n')\n",
    "f.close()\n",
    "print(\"create text and store information.\")\n",
    "\n",
    "\n",
    "## here the stored concepts will be further improved\n",
    "f = open(f\"full_concepts_stored_lists_{num3}_{num2}_improved_modify.txt\", \"a\")\n",
    "for ii in range(len(improved_concept_list)):\n",
    "    f.write(improved_concept_list[ii]+'\\n')\n",
    "f.close()\n",
    "print(\"create text and store information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fba71-5d7b-41ca-a68d-e4d7b06423b6",
   "metadata": {},
   "source": [
    "### further improve the concepts\n",
    "load the txt file, filter concepts, remove the orginal txt file, re-store the txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb85ff",
   "metadata": {},
   "source": [
    "1. 基础词汇过滤\n",
    "数字相关：过滤包含数字字符或数字单词（one, two等）的概念\n",
    "\n",
    "年份检测：使用正则表达式过滤纯年份概念\n",
    "\n",
    "通用停用词：filter_concept_lists 包含大量描述性词汇\n",
    "\n",
    "2. 字符串模式过滤\n",
    "特定字符：过滤包含 *, :, +, ], ; 等符号的概念\n",
    "\n",
    "短语模式：过滤常见无意义短语组合（\"low cost\", \"high cost\", \"n x n\"等）\n",
    "\n",
    "复杂短语：filter_contain_str_lists 包含81个需要过滤的特定短语模式\n",
    "\n",
    "3. 位置特定过滤\n",
    "开头词限制：\n",
    "\n",
    "通用开头词：filter_concept_start_words（187个）\n",
    "\n",
    "长度相关开头词：filter_concept_start_restricted_len（73个）\n",
    "\n",
    "结尾词限制：\n",
    "\n",
    "通用结尾词：filter_concept_last_words（108个）\n",
    "\n",
    "长度相关结尾词：filter_concept_last_restricted_len（36个）\n",
    "\n",
    "第二词限制：对3词以上概念检查第二个词\n",
    "\n",
    "4. 长度特定规则\n",
    "2词概念特殊处理：\n",
    "\n",
    "检查是否包含受限词汇\n",
    "\n",
    "验证开头和结尾词\n",
    "\n",
    "处理\"单词+数字\"模式\n",
    "\n",
    "检查单词长度组合合理性\n",
    "\n",
    "3词概念特殊处理：\n",
    "\n",
    "处理\"数字+数字+单词\"模式\n",
    "\n",
    "检查各位置单词长度组合\n",
    "\n",
    "5. 高级语义过滤\n",
    "重复结构检测：识别并过滤如\"word word\"这类重复结构\n",
    "\n",
    "低分词汇检测：\n",
    "\n",
    "开头低分词：如果去掉开头词后概念仍存在，则过滤原概念\n",
    "\n",
    "结尾低分词：如果去掉结尾词后概念仍存在，则过滤原概念\n",
    "\n",
    "复数形式处理：如果单数形式已存在，则过滤复数形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5591670b-615b-4785-bf8b-a111da0e3eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-12-2025 00:53:02; Concepts: 613794 \n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('full_concepts_stored_lists_6_9_improved_modify.txt'):\n",
    "    # open the existing pickle file for reading   \n",
    "    with open(\"full_concepts_stored_lists_6_9_improved_modify.txt\", \"r\") as f:\n",
    "        modify_full_concept_list = [line.rstrip() for line in f.readlines()]\n",
    "    \n",
    "    now_time =  datetime.datetime.now()\n",
    "    formatted_time = now_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    print(\"{}; Concepts: {:d} \".format(formatted_time,len(modify_full_concept_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c765f976-cd90-4fe4-aebe-c5894d62fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts: 613794 ; Store: 434771; Remove: 179023 \n",
      "Elapsed time: 315.98 seconds\n",
      "\n",
      "\n",
      "05-12-2025 00:56:17; Concepts: 434771 \n"
     ]
    }
   ],
   "source": [
    "filter_start_words_len2=['ci','order','resnet','theorem','level','le','ge','class','experiment','pm','figure',\n",
    "                          'type','dimension','cifar','characteristic','rank','probability','hiv','factor','top',\n",
    "                        'range','full','mass','central','lemma','ii','pythia','group','gsim']\n",
    "\n",
    "\n",
    "filter_start_words_len3=['map','galaxy','line','time','observation','tev','emission','transition','mpc','mk','min']\n",
    "\n",
    "\n",
    "### the start word of concept\n",
    "\n",
    "filter_concept_start_words=['saving','tracking','measuring','puzzling','injecting','interpreting','gaining','whilst',\n",
    "                           'inform','control','novel','improving','increase','unexpectedly','detecting','greatly','pm',\n",
    "                           'accelerate','promote','desired','introducing','decades','canadian','solving','advancing',\n",
    "                           'performing','showing','via','extract','losing','lose','representing','equiv','set','sets',\n",
    "                           'playing','play','rightarrow','scenario','ongoing','thorough','conduct','developing','carry',\n",
    "                           'setting','survey','powerful','development','estimating','corresponding','interesting','place',\n",
    "                           'never','address','addressing','overcome','overcoming','upcoming','forthcoming','values','neither',\n",
    "                           'understanding','total','kept','somewhat','somehow','training','proposing','reasonably','regularly',\n",
    "                           'containing','evolving','accelerating','decreases','read','check','checking','rigorously','reducing',\n",
    "                           'numerous','encoding','though','models','methods','method','dimensional','body','environmentally',\n",
    "                            'dimensionally','leveraging','aforementioned','escape','escaping','ethical','noticeable','track',\n",
    "                           'anomaly','surrounding','analysis','comprehensive','fairly','decays','project','firmly','effect',\n",
    "                           'showed','improved','maintaining','save','saves','rescue','answers','answering','relativistically',\n",
    "                           'simulating','outgoing','seen','transform','design','designed','assisting','assist','worlds',\n",
    "                           'fascinating','became','encourage','encourages','encouraging','findings','infect','infects','sharing',\n",
    "                           'intervening','langle','end','useful','latter','mathematically','problem','simpler','arguably',\n",
    "                           'computationally','expensive','technically','insufficient','insufficiently','constructing','repeat',\n",
    "                           'devices','guaranteed','violent','imposing','identifying','commonly','situation','dependently','papers',\n",
    "                           'societal','demonstrating','preliminary','test','conducting','deploying','deploy','primary','deepest',\n",
    "                           'architectural','plays','maps','infer','popular','background','preparing','attracting','attract',\n",
    "                           'attracts','aroused','measure','motivating','agricultural','putting','focuses','pay','pays','paying',\n",
    "                           'distribute','distributing','forecasted','capture','munich','existential','biggest','prominent',\n",
    "                           'supports','spectacular','samples','minimize','evaluating','methodswe','objectivewe','resultswe',\n",
    "                           'exploring','comparing','complicated','positions','key','capture','captures','irregularly',\n",
    "                            'surround','remaining']\n",
    "\n",
    "\n",
    "filter_concept_start_restricted_len=['average','optimal','subject','nontrivial','experimental','individual','large','exact',\n",
    "                                    'magnitude','expected','common','small','issue','description','empirical','lesssim',\n",
    "                                     'necessarily','drop','initial','model','environmental','difference','numerical','effective',\n",
    "                                    'non','modified','continued','enhanced','shortest','longest','throughout','parameters',\n",
    "                                     'generalized','electrically','extensive','size','simulated','selective','original','big',\n",
    "                                    'extended','trivial','perfectly','like','complementary','late','topologically','highest',\n",
    "                                    'human','totally','forward','random','produced','prescribed','impact','compactly','finitely',\n",
    "                                    'nearest','marked','repeating','defined','principle','widespread','generated','infinite','ancilla',\n",
    "                                    'third','second','unconventional','particular','greatest','great','actively','lossy','components',\n",
    "                                    'neighboring','smoothly','subsequent','equivalent','heavily','equivalently','reliable','route',\n",
    "                                    'stronger','restricted','core','points']\n",
    "\n",
    "### the last word of concept\n",
    "\n",
    "filter_concept_last_words=['t_','eta','saving','discus','constitutes','constitute','topic','mood','created','play',\n",
    "                          'mathematician','near','unique','experimental','carry','repository','generating','depending',\n",
    "                          'speaker','appearing','collaboration','drop','audience','lie','high','low','higher','lower',\n",
    "                          'computational','varying','relevant','containing','evolving','training','read','bar','check',\n",
    "                          'checking','initial','relating','driven','restricted','suffer','originates','originating',\n",
    "                          'presented','universal','modified','improved','suppressed','corresponding','stronger','large',\n",
    "                          'coupled','theoretical','dominated','extending','contained','nontrivial','applied','belonging',\n",
    "                          'belong','belongs','designed','collaborator','coder','alike','practitioners','measured','scattered',\n",
    "                          'end','fixed','correct','motivated','part','involved','investigator','guarantee','highland',\n",
    "                          'centric','produced','encountered','supported','represented','aimed','adopted','subtracted',\n",
    "                          'outside','inside','tend','tends','tended','tending','chooses','detail','concerning','defined',\n",
    "                          'defines','interact','interacts','interacted','loaded','vanishes','vanish','tendency','admitting',\n",
    "                          'prior','determined','located','indicated','ass','massive','implication','heated','surrounding',\n",
    "                          'surrounded','aligned','stemming','characterized']\n",
    "\n",
    "filter_concept_last_restricted_len=['subject','individual','large','exact','expected','common','small','issue','impact',\n",
    "                                   'description','numerical','faced','condition','community','reducing','route','visual',\n",
    "                                   'reduced','agreed','investigation','criterion','knowledge','k','noised','staff','server',\n",
    "                                   'induced','carried','developing','measuring','thought','water','size','launched','point']\n",
    "\n",
    "### the whole words of concept\n",
    "\n",
    "filter_concept_lists=['canada','eastern','shows','mainly','originate','tests','increasingly','ensuring','et','suppress',\n",
    "                     'enhance','facilitate','tracked','already','exists','exist','updated','governmental','handle','report',\n",
    "                     'berkeley','considered','considering','crucial','seems','seem','involves','involve','producing','produces',\n",
    "                     'acting','shed','studying','represents','diverges','reasonable',';\\\\','\\\\;','’','managing','exploit','everyday',\n",
    "                     'raising','||','[[','spent','yr','exceed','choose','meet','meeting','admits','admit','overcomes','known',\n",
    "                     'went','undergoes','approaches','true','generally','keep','keeps','trains','namely','played','assuming',\n",
    "                     'assume','assumes','propose','began','begin','grows','grow','growing','ever','enabling','exponentially',\n",
    "                     'slowly','varies','converges','converge','converging','arbitrarily','contains','evolves','accelerates',\n",
    "                     'dramatically','substantially','changes','increases','increase','degrade','degrades','alter','abruptly',\n",
    "                     'guaranteeing','guarantees','sufficient','establishing','corresponds','correspond','implies','relying',\n",
    "                     'rely','incorporate','incorporates','incorporating','rigorous','severity','schr','ultimate','ultimately',\n",
    "                     'encode','straightforward','conceptually','still','fail','fails','cycles','around','leverages','leverage',\n",
    "                     'dinger','introduction','analogue','disappears','disappear','disappearing','vienna','china','usa','japan',\n",
    "                      'europe','southern','east','west','north','south','french','america','longer','shorter','unseen','publisher',\n",
    "                     'combines','combine','combining','establishes','conclude','authors','author','research','build','builds',\n",
    "                     'building','bad','default','v','modify','modifies','whereby','arising','arise','gets','continue','continues',\n",
    "                     'continuing','highlight','highlights','replaced','getting','challenge','date','overview','maintain','london',\n",
    "                     'maintains','resetting','reset','upon','publication','elsewhere','journal','article','course','achieving',\n",
    "                     'reach','saving','maximum','coincide','coincides','coinciding','investigating','generalizing','classroom',\n",
    "                     'instruction','allowed','conducted','questions','question','acquired','acquires','acquiring','acquire',\n",
    "                     'simulate','discipline','laboratory','involved','dominate','dominates','involving','expertise','expert',\n",
    "                      'experts','underlying','guideline','recommended','harm','recommend','trivially','whatever','conducted',\n",
    "                     'scientist','scientists','interested','anyone','else','readers','reader','engineer','engineers','others',\n",
    "                     'appear','appearing','appeared','outperform','outperforming','outperformed','amongst','raise','raises',\n",
    "                     'infecting','publicly','released','share','shares','today','incorrect','incorrectly','health','wish',\n",
    "                      'alternative','appnl','le','th','supply','uncertain','constructed','certainty','imposed','identified',\n",
    "                     'identifies','waiting','\\\\\"','tool','equ','contributing','representing','represent','strategies','implemented',\n",
    "                     'behaviour','comparable','demonstrated','obtains','inefficient','female','male','exclusive','deployed',\n",
    "                     'outstanding','thesis','develops','obeys','obey','extracted','assign','assigns','assigned','able','interview',\n",
    "                     'student','prepared','prepare','prepares','teacher','teach','teaching','applications','asserts','assert',\n",
    "                     'interesting','theoretically','measures','motivation','motivate','motivates','come','comes','coming',\n",
    "                     'robustness','examine','examines','examining','examined','analyse','analyses','analysed','preventive',\n",
    "                     'illustristng','immediate','scientific','trusted','exclusively','primarily','asset','always','demanding',\n",
    "                     'fruitful','idea','captures','observatory','observing','threat','huge','burden','healthcare','smart',\n",
    "                     'want','win','uescher','terms','sample','drawn','draw','draws','regret','evaluated','evaluates','launch',\n",
    "                     'analyzed','regions','wit','refines','keeping','ensures','conditions','studies','suggested','sampled',\n",
    "                     'exhibits','exhibited','redundant','remained','infrastructure','treated','treat','treating','cache','align'] \n",
    "\n",
    "\n",
    "\n",
    "filter_concept_restricted_len=['north','western','see','saving','decrease','influence','question','simple','specific','essential',\n",
    "                              'ranked','answer','twitter','negligible','challenge','address','addressing','wrong','total','local',\n",
    "                              'high','low','higher','lower','largest','smallest','fast','slow','varying','altered','sharply',\n",
    "                              'direction','contribution','bar','missing','critical','critically','intrinsically','global','min',\n",
    "                              'environment','methodology','major','body','prescription','setting','basic','dataset','datasets',\n",
    "                              'theoretical','experimental','universal','near','proof','proofs','facility','actor','policy','cost'\n",
    "                              'conventional','suppressed','data','time','online','obstacle','avoiding','avoids','modern','strictly',\n",
    "                              'separate','separates','separating','minimally','minimal','maximum','maximally','maximal','spatially',\n",
    "                              'direct','privacy','parameter','resonantly','traditional','validation','weakly','sub','access','test',\n",
    "                              'age','across','frame','throughput','survey','independent','little','h','item','injury','distinct',\n",
    "                              'key','world','working','accessible','shared','merger','motivated','change','sharing','event','rate',\n",
    "                              'temperature','device','right','factor','joint','understanding','fashion','way','ways','activity',\n",
    "                              'related','bound','flat','histogram','regular','implementation','positive','realistic','efficient',\n",
    "                              'supported','normal','cover','type','types','enough','strategy','adopted','increased','controlled',\n",
    "                              'real','complex','player','region','architecture','driving','control','development','repeated','role',\n",
    "                              'design','interacting','assignment','observer','initially','application','interest','measure','position',\n",
    "                              'science','iteration','benchmarking','epidemic','area','viable','platform','flexible','domain',\n",
    "                              'assumed','trust','specifically','demand','classical','processing','forecasting','capture','database',\n",
    "                              'consistent','estimation','rising','prior','term','infinitely','determined','de','accuracy','prediction',\n",
    "                              'intermediate','strong','young','level','pathway','relationship','monitoring','enabled','alignment',\n",
    "                              'aligned','larger','limiting']\n",
    "\n",
    "\n",
    "\n",
    "### others\n",
    "\n",
    "filter_contain_str_lists=['meter telescope','sigma significance','derivative term','small enough','large enough','sigma level',\n",
    "                          'limit set','upper limit','large range','small range','time interval','web site','open research',\n",
    "                         'realistic device','large device','small device','lower limit','public health','building block',\n",
    "                         'realistic model','real model','real life','world health','human health','public health','major issue',\n",
    "                         'open issue','open problem','long standing','high level','low level','peer reviewed','computational cost',\n",
    "                         'open access','open question','negative answer','systematic numerical','careful analysis','high statistical',\n",
    "                          'numerical analysis','low quality','general dependence','large database','real word','effective method',\n",
    "                         'computational pipeline','strings ending','non trivial','general high','took place','exact solution',\n",
    "                         'explicit solution','high impact','numerical study','theoretical study','cost path','order term',\n",
    "                         'control parameter','weight loss','initial condition','high area','large area','small area','high accuracy',\n",
    "                          'high risk','particular condition','particular attention','event related','average level','key goal',\n",
    "                         'higher accuracy','highest accuracy','accurate theoretical','training dataset','key step','initial step',\n",
    "                         'strong solution','unique global','key feature','small limit','large limit']\n",
    "\n",
    "\n",
    "filter_contain_str=['low cost','high cost','n x n','*',':','+',']',';','throughput search']\n",
    "\n",
    "\n",
    "filter_concept_second_words=['experimental','challenge','data']\n",
    "\n",
    "    \n",
    "filter_concept_low_score_words=['underlying','normally','necessarily','usual','expected','efficient','relevant','strongly','realistic',\n",
    "                                'normal','simple','major','basic','surrounding','numerical','specific','experimental','conventional',\n",
    "                               'modified','modern','strictly','generalized','traditional','original','nontrivial','totally','particular',\n",
    "                                'increased','popular','initially','viable','flexible','unconventional','conventional','assumed','larger',\n",
    "                               'specifically','actively','smoothly','subsequent','reliable','initial','stronger','highest']\n",
    "\n",
    "filter_concept_low_score_words2=['model','method','technique','application','approach','community','framework','research',\n",
    "                                 'pipeline','literature','strategy','workflow','paradigm','tool','architecture',\n",
    "                                 'technology','methodology','prescription','pathway']\n",
    "\n",
    "number_char_list=['0','1','2','3','4','5','6','7','8','9']\n",
    "number_word_list=['one','two','three','four','five','six','seven','eight','nine','ten','eleven','twelve']\n",
    "\n",
    "       \n",
    "modify_improved_concept_list=[]\n",
    "    \n",
    "for one_concept in modify_full_concept_list:\n",
    "    \n",
    "    separated_words=one_concept.split()\n",
    "    do_remove=0\n",
    "    \n",
    "    for word in separated_words:\n",
    "        if word in number_word_list:\n",
    "            do_remove=1\n",
    "            break\n",
    "        \n",
    "        if word in filter_concept_lists: ## check whether contains to-remove words\n",
    "            do_remove=1\n",
    "            break\n",
    "        if is_valid_year(word): ## check whether it conatins a number as year\n",
    "            do_remove=1\n",
    "            break  \n",
    "          \n",
    "    if do_remove ==0:\n",
    "        for x in number_char_list:\n",
    "            if x in one_concept:\n",
    "                do_remove=1\n",
    "                break \n",
    "    \n",
    "    for ii in range(len(filter_contain_str)):  ## check whether it contains a certain string\n",
    "        if filter_contain_str[ii] in one_concept: \n",
    "            do_remove=1\n",
    "            break\n",
    "    \n",
    "    for filter_concepts in filter_contain_str_lists: ## check contain any of the two words\n",
    "        seperated_filter=filter_concepts.split()\n",
    "        counts=0\n",
    "        for word_filter in seperated_filter:\n",
    "            \n",
    "            if word_filter in separated_words: \n",
    "                counts+=1\n",
    "            if counts==len(seperated_filter):\n",
    "                do_remove=1\n",
    "                break\n",
    "                \n",
    "    if do_remove==0:   \n",
    "        \n",
    "        if len(separated_words)==2:  ## fix the length==2\n",
    "            for word in separated_words:\n",
    "                if word in filter_concept_restricted_len: #check whether it cantains to-remove words\n",
    "                    do_remove=1\n",
    "                    break\n",
    "\n",
    "            if separated_words[0] in filter_concept_start_restricted_len: #check the start word \n",
    "                do_remove=1\n",
    "            if separated_words[-1] in filter_concept_last_restricted_len: #check the last word \n",
    "                do_remove=1\n",
    "\n",
    "            # the first is a word, and the rest is a number\n",
    "            if separated_words[1].isnumeric() and separated_words[0] in filter_start_words_len2: \n",
    "                do_remove=1 \n",
    "                \n",
    "            if (len(separated_words[0])!=1 and len(separated_words[1])==1) or (len(separated_words[0])==1 and len(separated_words[1])!=1):\n",
    "                do_remove=1  \n",
    "\n",
    "\n",
    "        if len(separated_words)==3: ## fix the length==3\n",
    "            # the first two are number and the rest is a word   \n",
    "            if separated_words[0].isnumeric() and  separated_words[1].isnumeric() and separated_words[2] in filter_start_words_len3:\n",
    "                do_remove=1 \n",
    "                \n",
    "            if len(separated_words[0])!=1 and len(separated_words[1])==1 and len(separated_words[2])==1:\n",
    "                do_remove=1 \n",
    "            if len(separated_words[0])==1 and len(separated_words[1])!=1 and len(separated_words[2])==1:\n",
    "                do_remove=1    \n",
    "            if len(separated_words[0])!=1 and len(separated_words[1])==1 and len(separated_words[2])==1:\n",
    "                do_remove=1    \n",
    "            if len(separated_words[0])!=1 and len(separated_words[1])!=1 and len(separated_words[2])==1:\n",
    "                do_remove=1\n",
    "            if len(separated_words[0])!=1 and len(separated_words[1])==1 and len(separated_words[2])!=1:\n",
    "                do_remove=1\n",
    "    \n",
    "        #check the start word; no length restriction \n",
    "        if separated_words[0] in filter_concept_start_words:\n",
    "            do_remove=1\n",
    "\n",
    "         #check the last word; no length restriction   \n",
    "        if separated_words[-1] in filter_concept_last_words:\n",
    "            do_remove=1\n",
    "\n",
    "        if len(separated_words)%2==0:\n",
    "            half0=separated_words[0:int(len(separated_words)/2)]\n",
    "            half1=separated_words[int(len(separated_words)/2):int(len(separated_words))]\n",
    "            if half0[-1][-1]=='s':\n",
    "                half0[-1] = half0[-1][:-1]\n",
    "            if half1[-1][-1]=='s':\n",
    "                half1[-1] = half1[-1][:-1]\n",
    "            if half0 == half1:\n",
    "                 do_remove=1\n",
    "            \n",
    "    if do_remove==0:\n",
    "        \n",
    "        if len(separated_words)>=3:\n",
    "            if separated_words[1] in filter_concept_second_words:  #check the second word\n",
    "                do_remove=1\n",
    "\n",
    "            ## filter low score words \n",
    "            if separated_words[0] in filter_concept_low_score_words:  \n",
    "\n",
    "                the_rest_words  = separated_words[1:len(separated_words)]\n",
    "                no_low_words_concept=' '.join(the_rest_words)\n",
    "\n",
    "                if no_low_words_concept in modify_full_concept_list:\n",
    "                    do_remove=1\n",
    "\n",
    "            if separated_words[-1] in filter_concept_low_score_words2:\n",
    "                the_previous_words  = separated_words[0:len(separated_words)-1]\n",
    "                no_low_words_concept=' '.join(the_previous_words)\n",
    "                if no_low_words_concept in modify_full_concept_list:\n",
    "                    do_remove=1\n",
    "\n",
    "        if separated_words[-1][-1]==\"s\": ## plural\n",
    "            plural_string = one_concept[:-1]\n",
    "            if plural_string in modify_full_concept_list:\n",
    "                do_remove=1\n",
    "            \n",
    "        if do_remove==0:\n",
    "            modify_improved_concept_list.append(one_concept)\n",
    "        \n",
    "print(\"Concepts: {:d} ; Store: {:d}; Remove: {:d} \".format(len(modify_full_concept_list), len(modify_improved_concept_list),len(modify_full_concept_list)-len(modify_improved_concept_list)))\n",
    "elapsed_time = time.time() - starting_time\n",
    "print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "print('\\n')\n",
    "now_time =  datetime.datetime.now()\n",
    "formatted_time = now_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"{}; Concepts: {:d} \".format(formatted_time,len(modify_improved_concept_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0630b5",
   "metadata": {},
   "source": [
    "输出文件\n",
    "full_concepts_stored_lists_6_9_improved_modify.txt：包含所有经过完整清洗流程的最终关键词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c00413ee-79a2-4e5d-ab84-2b40534d27aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete the original txt file\n",
      "create a new txt file.\n",
      "05-12-2025 00:58:30; Concepts: 434771 \n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"full_concepts_stored_lists_6_9_improved_modify.txt\"):\n",
    "    os.remove(\"full_concepts_stored_lists_6_9_improved_modify.txt\")\n",
    "    print(\"delete the original txt file\")\n",
    "    \n",
    "    f = open(\"full_concepts_stored_lists_6_9_improved_modify.txt\", \"a\")\n",
    "    for ii in range(len(modify_improved_concept_list)):\n",
    "        f.write(modify_improved_concept_list[ii]+'\\n')\n",
    "    f.close()\n",
    "    print(\"create a new txt file.\")\n",
    "    \n",
    "else:\n",
    "    f = open(\"full_concepts_stored_lists_6_9_improved_modify.txt\", \"a\")\n",
    "    for ii in range(len(modify_improved_concept_list)):\n",
    "        f.write(modify_improved_concept_list[ii]+'\\n')\n",
    "    f.close()\n",
    "    print(\"create text and store information.\")\n",
    "    \n",
    "now_time =  datetime.datetime.now()\n",
    "formatted_time = now_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"{}; Concepts: {:d} \".format(formatted_time,len(modify_improved_concept_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8baa4ff9-d2a3-4e7b-b9a9-99df51301388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just a backup of the full concepts\n",
    "with open(\"full_concept_list.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(modify_improved_concept_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c78cf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts saved to full_concept_list.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 加载 .pkl 文件\n",
    "with open(\"full_concept_list.pkl\", \"rb\") as input_file:\n",
    "    full_concept_list = pickle.load(input_file)\n",
    "\n",
    "# 将关键词写入 .txt 文件\n",
    "with open(\"full_concept_list.txt\", \"w\") as f:\n",
    "    for concept in full_concept_list:\n",
    "        f.write(concept + \"\\n\")\n",
    "\n",
    "print(\"Concepts saved to full_concept_list.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
